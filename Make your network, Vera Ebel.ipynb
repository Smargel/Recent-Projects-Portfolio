{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\api.py:107: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "#based on https://www.quora.com/How-can-l-visualize-cifar-10-data-RGB-using-python-matplotlib\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict1 = pickle.load(fo, encoding='bytes')\n",
    "    return dict1\n",
    "#Create the dataframes \n",
    "pd_tr = pd.DataFrame()\n",
    "tr_y = pd.DataFrame()\n",
    "#Reading files 1-5\n",
    "for i in range(1,6):\n",
    "    data = unpickle(r\"C:\\Users\\Ebel\\Downloads\\cifar-10-python.tar\\cifar-10-batches-py\\data_batch_\" + str(i))\n",
    "    #Place the data matrix as the training data \n",
    "    pd_tr = pd_tr.append(pd.DataFrame(data[b'data']))\n",
    "    #Read the data matrix labels as the Y variable\n",
    "    tr_y = tr_y.append(pd.DataFrame(data[b'labels']))\n",
    "    #Turning the dataframe of the dataframe into a column of the original dataframe, merging them. \n",
    "    pd_tr['labels'] = tr_y\n",
    "\n",
    "tr_x = np.asarray(pd_tr.iloc[:, :3072])\n",
    "tr_y = np.asarray(pd_tr['labels'])\n",
    "#Reading the test batch as a test data frame. \n",
    "ts_x = np.asarray(unpickle(r\"C:\\Users\\Ebel\\Downloads\\cifar-10-python.tar\\cifar-10-batches-py\\test_batch\")[b'data'])\n",
    "ts_y = np.asarray(unpickle(r\"C:\\Users\\Ebel\\Downloads\\cifar-10-python.tar\\cifar-10-batches-py\\test_batch\")[b'labels'])    \n",
    "labels = unpickle(r\"C:\\Users\\Ebel\\Downloads\\cifar-10-python.tar\\cifar-10-batches-py\\batches.meta\")[b'label_names']\n",
    "\n",
    "def plot_CIFAR(ind):\n",
    "    #Turn the index of the training data into an array. \n",
    "    arr = tr_x[ind]\n",
    "    #Reshaping each RGB section into their 32*32 respective matrixes, making them representative of the pixels \n",
    "    #they're supposed to represent.\n",
    "    R = arr[0:1024].reshape(32,32)/255.0\n",
    "    G = arr[1024:2048].reshape(32,32)/255.0\n",
    "    B = arr[2048:].reshape(32,32)/255.0\n",
    "    #Plotting out the pixels into an image for testing \n",
    "    #making RGB stack on top of eachother, producing the spectrum of color. \n",
    "    img = np.dstack((R,G,B))\n",
    "    title = re.sub('[!@#$b]', '', str(labels[tr_y[ind]]))\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img,interpolation='bicubic')\n",
    "    ax.set_title('Category = '+ title,fontsize =15)\n",
    "#Ploting 4th image in the data set. \n",
    "plot_CIFAR(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That solved the issue of importing the images, and getting them partially vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames']) \n",
      "\n",
      "b'batch_label' <class 'bytes'> 21\n",
      "b'labels' <class 'list'> 10000\n",
      "b'data' <class 'numpy.ndarray'> 10000\n",
      "b'filenames' <class 'list'> 10000\n"
     ]
    }
   ],
   "source": [
    "pd_tr = pd.DataFrame()\n",
    "tr_y = pd.DataFrame()\n",
    "\n",
    "file = r\"C:\\Users\\Ebel\\Downloads\\cifar-10-python.tar\\cifar-10-batches-py\\data_batch_1\"\n",
    "with open(file, 'rb') as fo:\n",
    "    dict1 = pickle.load(fo, encoding='bytes')\n",
    "print(dict1.keys(), '\\n')\n",
    "for key in dict1.keys():\n",
    "    print(key, type(dict1[key]), len(dict1[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3073)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>250</td>\n",
       "      <td>238</td>\n",
       "      <td>233</td>\n",
       "      <td>245</td>\n",
       "      <td>241</td>\n",
       "      <td>232</td>\n",
       "      <td>238</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>231</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>126</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>173</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205</td>\n",
       "      <td>213</td>\n",
       "      <td>235</td>\n",
       "      <td>232</td>\n",
       "      <td>112</td>\n",
       "      <td>98</td>\n",
       "      <td>95</td>\n",
       "      <td>80</td>\n",
       "      <td>98</td>\n",
       "      <td>224</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189</td>\n",
       "      <td>184</td>\n",
       "      <td>181</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>177</td>\n",
       "      <td>186</td>\n",
       "      <td>167</td>\n",
       "      <td>147</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>77</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...    3063  3064  3065  \\\n",
       "0  255  252  253  250  238  233  245  241  232  238   ...     253   253   252   \n",
       "1  127  126  127  127  128  128  128  128  129  130   ...     103    97    95   \n",
       "2  116   64   19   29   36   40   57  143  173   83   ...      32    17    10   \n",
       "3  205  213  235  232  112   98   95   80   98  224   ...      23    25    25   \n",
       "4  189  184  181  186  191  177  186  167  147  161   ...      75    77    74   \n",
       "\n",
       "   3066  3067  3068  3069  3070  3071  labels  \n",
       "0   252   231   172   173   231   248       1  \n",
       "1    88    81    89   102   108   112       8  \n",
       "2     9    10     8     7     6     5       5  \n",
       "3    26    23    23    24    25    23       1  \n",
       "4    75    76    72    76    77    72       5  \n",
       "\n",
       "[5 rows x 3073 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tr = pd_tr.append(pd.DataFrame(data[b'data']))\n",
    "tr_y = tr_y.append(pd.DataFrame(data[b'labels']))\n",
    "pd_tr['labels'] = tr_y\n",
    "print(pd_tr.shape)\n",
    "pd_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3073)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>68</td>\n",
       "      <td>98</td>\n",
       "      <td>119</td>\n",
       "      <td>139</td>\n",
       "      <td>145</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>65</td>\n",
       "      <td>59</td>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>104</td>\n",
       "      <td>140</td>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>126</td>\n",
       "      <td>105</td>\n",
       "      <td>102</td>\n",
       "      <td>125</td>\n",
       "      <td>155</td>\n",
       "      <td>172</td>\n",
       "      <td>180</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>67</td>\n",
       "      <td>101</td>\n",
       "      <td>122</td>\n",
       "      <td>133</td>\n",
       "      <td>136</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>144</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>69</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>59</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>168</td>\n",
       "      <td>177</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>184</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>79</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>159</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>138</td>\n",
       "      <td>184</td>\n",
       "      <td>154</td>\n",
       "      <td>77</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>164</td>\n",
       "      <td>105</td>\n",
       "      <td>118</td>\n",
       "      <td>129</td>\n",
       "      <td>134</td>\n",
       "      <td>146</td>\n",
       "      <td>166</td>\n",
       "      <td>183</td>\n",
       "      <td>199</td>\n",
       "      <td>174</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>42</td>\n",
       "      <td>55</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>110</td>\n",
       "      <td>97</td>\n",
       "      <td>113</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>128</td>\n",
       "      <td>133</td>\n",
       "      <td>139</td>\n",
       "      <td>140</td>\n",
       "      <td>134</td>\n",
       "      <td>121</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>...</td>\n",
       "      <td>233</td>\n",
       "      <td>224</td>\n",
       "      <td>208</td>\n",
       "      <td>145</td>\n",
       "      <td>116</td>\n",
       "      <td>128</td>\n",
       "      <td>136</td>\n",
       "      <td>137</td>\n",
       "      <td>138</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>125</td>\n",
       "      <td>110</td>\n",
       "      <td>102</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>141</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>148</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>75</td>\n",
       "      <td>79</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...    3063  3064  3065  \\\n",
       "0   59   43   50   68   98  119  139  145  149  149   ...      58    65    59   \n",
       "1  154  126  105  102  125  155  172  180  142  111   ...      42    67   101   \n",
       "2  255  253  253  253  253  253  253  253  253  253   ...      83    80    69   \n",
       "3   28   37   38   42   44   40   40   24   32   43   ...      39    59    42   \n",
       "4  170  168  177  183  181  177  181  184  189  189   ...      88    85    82   \n",
       "5  159  150  153  154  138  184  154   77   61   64   ...      18    16    12   \n",
       "6  164  105  118  129  134  146  166  183  199  174   ...      71    48    58   \n",
       "7   28   30   33   62   63   31   29   42   55   67   ...      83   107   110   \n",
       "8  134  131  128  133  139  140  134  121  124  124   ...     233   224   208   \n",
       "9  125  110  102  106  106  141  175  175  148  106   ...      62    67    70   \n",
       "\n",
       "   3066  3067  3068  3069  3070  3071  labels  \n",
       "0    46    57   104   140    84    72       6  \n",
       "1   122   133   136   139   142   144       9  \n",
       "2    66    72    79    83    83    84       9  \n",
       "3    44    48    38    28    37    46       4  \n",
       "4    83    79    78    82    78    80       1  \n",
       "5    13    16    14    14    17    19       1  \n",
       "6    64    48    41    29    26    44       2  \n",
       "7    97   113   117   100    99    96       7  \n",
       "8   145   116   128   136   137   138       8  \n",
       "9    75    79    81    82    84    86       3  \n",
       "\n",
       "[10 rows x 3073 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tr = pd.DataFrame()\n",
    "tr_y = pd.DataFrame()\n",
    "\n",
    "for i in range(1,5):\n",
    "    data = unpickle(r\"C:\\Users\\Ebel\\Downloads\\cifar-10-python.tar\\cifar-10-batches-py\\data_batch_\" + str(i))\n",
    "    pd_tr = pd_tr.append(pd.DataFrame(data[b'data']))\n",
    "    tr_y = tr_y.append(pd.DataFrame(data[b'labels']))\n",
    "    pd_tr['labels'] = tr_y\n",
    "print(pd_tr.shape)\n",
    "pd_tr.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically this created a test and training set on it's own. But I found the processing to be very intense without the 20% increase in DF size, so I split the training data frame into new train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd_tr.loc[:, pd_tr.columns != 'labels'], pd_tr['labels'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first want to experiment a bit with a MLPClassifier to see how well a normal network does in a supervised envioerment with this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000))\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's supicsious... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3062</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>115</td>\n",
       "      <td>156</td>\n",
       "      <td>223</td>\n",
       "      <td>227</td>\n",
       "      <td>219</td>\n",
       "      <td>220</td>\n",
       "      <td>221</td>\n",
       "      <td>217</td>\n",
       "      <td>216</td>\n",
       "      <td>221</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>125</td>\n",
       "      <td>136</td>\n",
       "      <td>130</td>\n",
       "      <td>128</td>\n",
       "      <td>120</td>\n",
       "      <td>109</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>220</td>\n",
       "      <td>191</td>\n",
       "      <td>194</td>\n",
       "      <td>196</td>\n",
       "      <td>193</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>191</td>\n",
       "      <td>189</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>113</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>154</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>84</td>\n",
       "      <td>126</td>\n",
       "      <td>139</td>\n",
       "      <td>135</td>\n",
       "      <td>132</td>\n",
       "      <td>145</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>202</td>\n",
       "      <td>204</td>\n",
       "      <td>205</td>\n",
       "      <td>202</td>\n",
       "      <td>158</td>\n",
       "      <td>133</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>137</td>\n",
       "      <td>157</td>\n",
       "      <td>144</td>\n",
       "      <td>158</td>\n",
       "      <td>159</td>\n",
       "      <td>173</td>\n",
       "      <td>165</td>\n",
       "      <td>168</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>109</td>\n",
       "      <td>112</td>\n",
       "      <td>124</td>\n",
       "      <td>125</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3072 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...   3062  \\\n",
       "9545   115   156   223   227   219   220   221   217   216   221  ...    138   \n",
       "394    220   191   194   196   193   192   198   191   189   185  ...     80   \n",
       "2926    60    65    65    84   126   139   135   132   145   110  ...      5   \n",
       "9905   202   204   205   202   158   133   135   135   143   142  ...    162   \n",
       "6524   123   130   137   157   144   158   159   173   165   168  ...     94   \n",
       "\n",
       "      3063  3064  3065  3066  3067  3068  3069  3070  3071  \n",
       "9545   134   139   125   136   130   128   120   109   116  \n",
       "394     76    79    69    67   113   140   149   154   157  \n",
       "2926     3    12     7     2     1     2     4     6     7  \n",
       "9905   162   164   166   166   173   174   173   173   178  \n",
       "6524    96   109   112   124   125   116   101   110    92  \n",
       "\n",
       "[5 rows x 3072 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9545    1\n",
       "394     5\n",
       "2926    5\n",
       "9905    1\n",
       "6524    2\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first want to experiment a bit with a MLPClassifier to see how well a normal network does in a supervised envioerment with this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's about as high as I could get it in a reasonable amount of time spent compiling. I think that's not as bad as it seems, given that there are ten potential labels to attach to an image, so if we were guessing completely randomly we could expect a result around 10%. Still, it's not accurate enough to be useful. Regardless, I'll move on to feature extraction, then using a traditional classification model following it and see where that takes me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I considered the argument about not normalizing bounded data and preformed both the RBM + RFC pipeline and regular RFC on non-normalized data, The result is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.3, n_components=256, n_iter=10,\n",
       "       random_state=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "RBM = BernoulliRBM(n_components=256, learning_rate=.3)\n",
    "RBM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10,\n",
       " 'learning_rate': 0.3,\n",
       " 'n_components': 256,\n",
       " 'n_iter': 10,\n",
       " 'random_state': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
       "       -1.01020100e+14, -0.00000000e+00, -1.68231521e+14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.3, n_components=256, n_iter=10,\n",
       "       random_state=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RTC = RandomForestClassifier(n_estimators=75, max_features=None)\n",
    "rbm_class_generator = Pipeline(steps=[('RBM', RBM), ('Random_Tree_Classifier', RTC)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('RBM', BernoulliRBM(batch_size=10, learning_rate=0.3, n_components=256, n_iter=10,\n",
       "       random_state=None, verbose=0)), ('Random_Tree_Classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm_class_generator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.3, n_components=256, n_iter=10,\n",
       "       random_state=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "RBM = BernoulliRBM(n_components=256, learning_rate=.3)\n",
    "RBM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3062</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>115</td>\n",
       "      <td>156</td>\n",
       "      <td>223</td>\n",
       "      <td>227</td>\n",
       "      <td>219</td>\n",
       "      <td>220</td>\n",
       "      <td>221</td>\n",
       "      <td>217</td>\n",
       "      <td>216</td>\n",
       "      <td>221</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>125</td>\n",
       "      <td>136</td>\n",
       "      <td>130</td>\n",
       "      <td>128</td>\n",
       "      <td>120</td>\n",
       "      <td>109</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>220</td>\n",
       "      <td>191</td>\n",
       "      <td>194</td>\n",
       "      <td>196</td>\n",
       "      <td>193</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>191</td>\n",
       "      <td>189</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>113</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>154</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>84</td>\n",
       "      <td>126</td>\n",
       "      <td>139</td>\n",
       "      <td>135</td>\n",
       "      <td>132</td>\n",
       "      <td>145</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>202</td>\n",
       "      <td>204</td>\n",
       "      <td>205</td>\n",
       "      <td>202</td>\n",
       "      <td>158</td>\n",
       "      <td>133</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>123</td>\n",
       "      <td>130</td>\n",
       "      <td>137</td>\n",
       "      <td>157</td>\n",
       "      <td>144</td>\n",
       "      <td>158</td>\n",
       "      <td>159</td>\n",
       "      <td>173</td>\n",
       "      <td>165</td>\n",
       "      <td>168</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>109</td>\n",
       "      <td>112</td>\n",
       "      <td>124</td>\n",
       "      <td>125</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3072 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...   3062  \\\n",
       "9545   115   156   223   227   219   220   221   217   216   221  ...    138   \n",
       "394    220   191   194   196   193   192   198   191   189   185  ...     80   \n",
       "2926    60    65    65    84   126   139   135   132   145   110  ...      5   \n",
       "9905   202   204   205   202   158   133   135   135   143   142  ...    162   \n",
       "6524   123   130   137   157   144   158   159   173   165   168  ...     94   \n",
       "\n",
       "      3063  3064  3065  3066  3067  3068  3069  3070  3071  \n",
       "9545   134   139   125   136   130   128   120   109   116  \n",
       "394     76    79    69    67   113   140   149   154   157  \n",
       "2926     3    12     7     2     1     2     4     6     7  \n",
       "9905   162   164   166   166   173   174   173   173   178  \n",
       "6524    96   109   112   124   125   116   101   110    92  \n",
       "\n",
       "[5 rows x 3072 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9545    1\n",
       "394     5\n",
       "2926    5\n",
       "9905    1\n",
       "6524    2\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's about as high as I could get it in a reasonable amount of time spent compiling. I think that's not as bad as it seems, given that there are ten potential labels to attach to an image, so if we were guessing completely randomly we could expect a result around 10%. Still, it's not accurate enough to be useful. Regardless, I'll move on to feature extraction, then using a traditional classification model following it and see where that takes me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10,\n",
       " 'learning_rate': 0.3,\n",
       " 'n_components': 256,\n",
       " 'n_iter': 10,\n",
       " 'random_state': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00, -5.20853617e+13, -0.00000000e+00, ...,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       784\n",
      "          1       0.00      0.00      0.00       760\n",
      "          2       0.00      0.00      0.00       815\n",
      "          3       0.00      0.00      0.00       797\n",
      "          4       0.00      0.00      0.00       850\n",
      "          5       0.00      0.00      0.00       806\n",
      "          6       0.10      1.00      0.17       766\n",
      "          7       0.00      0.00      0.00       805\n",
      "          8       0.00      0.00      0.00       828\n",
      "          9       0.00      0.00      0.00       789\n",
      "\n",
      "avg / total       0.01      0.10      0.02      8000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn. preprocessing import normalize\n",
    "Y_pred = rbm_class_generator.predict(X_test)\n",
    "print(\"Random Forest Classification using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model isn't getting any precisions except on a single category. In fact, it seems like it's possible that it's marking literally everything as \"8,\" and 10% happened to be right given the distribution of the dataset labels. I think I might have to either adjust the model or the pipeline that fed into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.3, n_components=256, n_iter=10,\n",
       "       random_state=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RTC = RandomForestClassifier(n_estimators=75, max_features=None)\n",
    "rbm_class_generator = Pipeline(steps=[('RBM', RBM), ('Random_Tree_Classifier', RTC)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('RBM', BernoulliRBM(batch_size=10, learning_rate=0.3, n_components=256, n_iter=10,\n",
       "       random_state=None, verbose=0)), ('Random_Tree_Classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm_class_generator.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.36      0.36       784\n",
      "          1       0.21      0.25      0.23       760\n",
      "          2       0.22      0.18      0.20       815\n",
      "          3       0.14      0.11      0.13       797\n",
      "          4       0.23      0.21      0.22       850\n",
      "          5       0.22      0.21      0.21       806\n",
      "          6       0.26      0.27      0.26       766\n",
      "          7       0.23      0.23      0.23       805\n",
      "          8       0.42      0.43      0.43       828\n",
      "          9       0.29      0.36      0.32       789\n",
      "\n",
      "avg / total       0.26      0.26      0.26      8000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "X_normalized = normalize(X_test)\n",
    "Y_pred = rbm_class_generator.predict(X_normalized)\n",
    "print(\"Random Forest Classification using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data were normalized, results were better then when data was not normalized, but distinctly worse than a simple Random Forest applied to non normalized data. It basically preforms as accurately as you would expect random guessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RBM = BernoulliRBM(n_components=256, learning_rate=.015, n_iter = 20, batch_size=100, verbose =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RTC = RandomForestClassifier(n_estimators=75, max_features=None)\n",
    "rbm_class_generator = Pipeline(steps=[('RBM', RBM), ('Random_Tree_Classifier', RTC)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -76.00, time = 17.41s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -57.43, time = 21.20s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -57.59, time = 21.20s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -57.71, time = 21.33s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -57.61, time = 21.30s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -57.47, time = 21.28s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -57.66, time = 21.37s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -57.51, time = 21.39s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -57.53, time = 21.42s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -57.64, time = 21.34s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -57.48, time = 21.36s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -57.63, time = 21.12s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -57.56, time = 21.18s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -57.62, time = 21.27s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -57.57, time = 21.10s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -57.58, time = 21.52s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -57.73, time = 21.18s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -57.50, time = 21.23s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -57.92, time = 21.38s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -57.43, time = 21.17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('RBM', BernoulliRBM(batch_size=100, learning_rate=0.015, n_components=256, n_iter=20,\n",
       "       random_state=None, verbose=1)), ('Random_Tree_Classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=No...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm_class_generator.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       784\n",
      "          1       0.00      0.00      0.00       760\n",
      "          2       0.10      1.00      0.18       815\n",
      "          3       0.00      0.00      0.00       797\n",
      "          4       0.00      0.00      0.00       850\n",
      "          5       0.00      0.00      0.00       806\n",
      "          6       0.00      0.00      0.00       766\n",
      "          7       0.00      0.00      0.00       805\n",
      "          8       0.00      0.00      0.00       828\n",
      "          9       0.00      0.00      0.00       789\n",
      "\n",
      "avg / total       0.01      0.10      0.02      8000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_normalized = normalize(X_test)\n",
    "Y_pred = rbm_class_generator.predict(X_test)\n",
    "print(\"Random Forest Classification using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because I'm interested in getting your input on this, and because it's not hard to tack on, I'll complete the random forest assignment here, by running a computationally intensive model with no limitations, then a very inaccurate model with lots of limitations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classification predicting labels raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.51      0.51       784\n",
      "          1       0.51      0.55      0.53       760\n",
      "          2       0.35      0.34      0.35       815\n",
      "          3       0.33      0.28      0.30       797\n",
      "          4       0.41      0.41      0.41       850\n",
      "          5       0.37      0.33      0.35       806\n",
      "          6       0.44      0.52      0.47       766\n",
      "          7       0.51      0.47      0.49       805\n",
      "          8       0.56      0.56      0.56       828\n",
      "          9       0.47      0.50      0.49       789\n",
      "\n",
      "avg / total       0.44      0.45      0.44      8000\n",
      "\n",
      "\n",
      "5300.745626211166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "import time \n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "raw_classifier = clone(RTC)\n",
    "\n",
    "raw_classifier.fit(X_train, y_train)\n",
    "Y_pred = raw_classifier.predict(X_test)\n",
    "print(\"Random forest classification predicting labels raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, Y_pred)))\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best I got, which was without the RBM features. Not ideal. The remaining attempts can be seen here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classification predicting labels raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.10      1.00      0.18       784\n",
      "          1       0.00      0.00      0.00       760\n",
      "          2       0.00      0.00      0.00       815\n",
      "          3       0.00      0.00      0.00       797\n",
      "          4       0.00      0.00      0.00       850\n",
      "          5       0.00      0.00      0.00       806\n",
      "          6       0.00      0.00      0.00       766\n",
      "          7       0.00      0.00      0.00       805\n",
      "          8       0.00      0.00      0.00       828\n",
      "          9       0.00      0.00      0.00       789\n",
      "\n",
      "avg / total       0.01      0.10      0.02      8000\n",
      "\n",
      "\n",
      "5001.267338752747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "import time \n",
    "start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "raw_classifier = clone(RTC)\n",
    "\n",
    "raw_classifier.fit(X_train, y_train)\n",
    "Y_pred = raw_classifier.predict(X_normalized)\n",
    "print(\"Random forest classification predicting labels raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, Y_pred)))\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I think I need to apply different models to the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -74.37, time = 18.11s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -57.54, time = 22.66s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -57.69, time = 22.14s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -57.84, time = 21.98s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -57.25, time = 22.29s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -57.70, time = 22.54s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -57.90, time = 22.47s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -57.86, time = 22.21s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -57.47, time = 21.89s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -57.54, time = 22.01s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -57.64, time = 22.03s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -57.46, time = 21.86s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -57.43, time = 21.91s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -57.56, time = 21.98s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -57.89, time = 21.99s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -57.66, time = 22.27s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -57.69, time = 22.44s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -57.86, time = 22.53s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -57.77, time = 22.81s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -57.51, time = 22.82s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('RBM', BernoulliRBM(batch_size=100, learning_rate=0.015, n_components=256, n_iter=20,\n",
       "       random_state=None, verbose=1)), ('Logistic_Regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "LogR = LogisticRegression(solver='lbfgs', max_iter=10000, multi_class='multinomial')\n",
    "RBM = BernoulliRBM(n_components=256, learning_rate=.015, n_iter = 20, batch_size=100, verbose =1 )\n",
    "\n",
    "Log_rbm_class_generator = Pipeline(steps=[('RBM', RBM), ('Logistic_Regression', LogR)])\n",
    "Log_rbm_class_generator.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measures of pseudo-likelihood are not encouraging. We'll see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.10      1.00      0.18       784\n",
      "          1       0.00      0.00      0.00       760\n",
      "          2       0.00      0.00      0.00       815\n",
      "          3       0.00      0.00      0.00       797\n",
      "          4       0.00      0.00      0.00       850\n",
      "          5       0.00      0.00      0.00       806\n",
      "          6       0.00      0.00      0.00       766\n",
      "          7       0.00      0.00      0.00       805\n",
      "          8       0.00      0.00      0.00       828\n",
      "          9       0.00      0.00      0.00       789\n",
      "\n",
      "avg / total       0.01      0.10      0.02      8000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X_normalized = normalize(X_test)\n",
    "Y_pred = Log_rbm_class_generator.predict(X_test)\n",
    "print(\"Random Forest Classification using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think, while I completed the assignment, technically, I didn't get the performance necessary to call this a success. I've tuned the hyperparameters, I've done what I think is appropriate for the dataset. I think I might have to do more preprocessing to actually get good results out of this. I'll leave it be for now and move on to other things. I may have a better approach later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If found this class and methodology, I thought would get a better defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\Ebel\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "# start-snippet-1\n",
    "class RBM(object):\n",
    "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RBM constructor. Defines the parameters of the model along with\n",
    "        basic operations for inferring hidden from visible (and vice-versa),\n",
    "        as well as for performing CD updates.\n",
    "        :param input: None for standalone RBMs or symbolic variable if RBM is\n",
    "        part of a larger graph.\n",
    "        :param n_visible: number of visible units\n",
    "        :param n_hidden: number of hidden units\n",
    "        :param W: None for standalone RBMs or symbolic variable pointing to a\n",
    "        shared weight matrix in case RBM is part of a DBN network; in a DBN,\n",
    "        the weights are shared between RBMs and layers of a MLP\n",
    "        :param hbias: None for standalone RBMs or symbolic variable pointing\n",
    "        to a shared hidden units bias vector in case RBM is part of a\n",
    "        different network\n",
    "        :param vbias: None for standalone RBMs or a symbolic variable\n",
    "        pointing to a shared visible units bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        if numpy_rng is None:\n",
    "            # create a number generator\n",
    "            numpy_rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        if theano_rng is None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        if W is None:\n",
    "            # W is initialized with `initial_W` which is uniformely\n",
    "            # sampled from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible)) the output of uniform if\n",
    "            # converted using asarray to dtype theano.config.floatX so\n",
    "            # that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            # theano shared variables for weights and biases\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if hbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            hbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='hbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if vbias is None:\n",
    "            # create shared variable for visible units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='vbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "\n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.theano_rng = theano_rng\n",
    "        # **** WARNING: It is not a good idea to put things in this list\n",
    "        # other than shared variables created in this function.\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "        # end-snippet-1\n",
    "\n",
    "    def free_energy(self, v_sample):\n",
    "        ''' Function to compute the free energy '''\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        #hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        hidden_term = T.sum(T.max(0, wx_b), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    def propup(self, vis):\n",
    "        '''This function propagates the visible units activation upwards to\n",
    "        the hidden units\n",
    "        Note that we return also the pre-sigmoid activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        ''' This function infers state of hidden units given visible units '''\n",
    "        # compute the activation of the hidden units given a sample of\n",
    "        # the visibles\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        # get a sample of the hiddens given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        '''This function propagates the hidden units activation downwards to\n",
    "        the visible units\n",
    "        Note that we return also the pre_sigmoid_activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        ''' This function infers state of visible units given hidden units '''\n",
    "        # compute the activation of the visible given the hidden sample\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        # get a sample of the visible given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the hidden state'''\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the visible state'''\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    # start-snippet-2\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        \"\"\"This functions implements one step of CD-k or PCD-k\n",
    "        :param lr: learning rate used to train the RBM\n",
    "        :param persistent: None for CD. For PCD, shared variable\n",
    "            containing old state of Gibbs chain. This must be a shared\n",
    "            variable of size (batch size, number of hidden units).\n",
    "        :param k: number of Gibbs steps to do in CD-k/PCD-k\n",
    "        Returns a proxy for the cost and the updates dictionary. The\n",
    "        dictionary contains the update rules for weights and biases but\n",
    "        also an update of the shared variable used to store the persistent\n",
    "        chain, if one is used.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive phase\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "\n",
    "        # decide how to initialize persistent chain:\n",
    "        # for CD, we use the newly generate hidden sample\n",
    "        # for PCD, we initialize from the old state of the chain\n",
    "        if persistent is None:\n",
    "            chain_start = ph_sample\n",
    "        else:\n",
    "            chain_start = persistent\n",
    "        # end-snippet-2\n",
    "        # perform actual negative phase\n",
    "        # in order to implement CD-k/PCD-k we need to scan over the\n",
    "        # function that implements one gibbs step k times.\n",
    "        # Read Theano tutorial on scan for more information :\n",
    "        # http://deeplearning.net/software/theano/library/scan.html\n",
    "        # the scan will return the entire Gibbs chain\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            # the None are place holders, saying that\n",
    "            # chain_start is the initial state corresponding to the\n",
    "            # 6th output\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k\n",
    "        )\n",
    "        # start-snippet-3\n",
    "        # determine gradients on RBM parameters\n",
    "        # note that we only need the sample at the end of the chain\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(\n",
    "            self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "        # end-snippet-3 start-snippet-4\n",
    "        # constructs the update dictionary\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(\n",
    "                lr,\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        if persistent:\n",
    "            # Note that this works only if persistent is a shared variable\n",
    "            updates[persistent] = nh_samples[-1]\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(updates,\n",
    "                                                           pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "        # end-snippet-4\n",
    "\n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n",
    "\n",
    "        # index of bit i in expression p(x_i | x_{\\i})\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n",
    "        # the result to xi_flip, instead of working in place on xi.\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n",
    "                                                            fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n",
    "        \"\"\"Approximation to the reconstruction error\n",
    "        Note that this function requires the pre-sigmoid activation as\n",
    "        input.  To understand why this is so you need to understand a\n",
    "        bit about how Theano works. Whenever you compile a Theano\n",
    "        function, the computational graph that you pass as input gets\n",
    "        optimized for speed and stability.  This is done by changing\n",
    "        several parts of the subgraphs with others.  One such\n",
    "        optimization expresses terms of the form log(sigmoid(x)) in\n",
    "        terms of softplus.  We need this optimization for the\n",
    "        cross-entropy since sigmoid of numbers larger than 30. (or\n",
    "        even less then that) turn to 1. and numbers smaller than\n",
    "        -30. turn to 0 which in terms will force theano to compute\n",
    "        log(0) and therefore we will get either -inf or NaN as\n",
    "        cost. If the value is expressed in terms of softplus we do not\n",
    "        get this undesirable behaviour. This optimization usually\n",
    "        works fine, but here we have a special case. The sigmoid is\n",
    "        applied inside the scan op, while the log is\n",
    "        outside. Therefore Theano will only see log(scan(..)) instead\n",
    "        of log(sigmoid(..)) and will not apply the wanted\n",
    "        optimization. We can not go and replace the sigmoid in scan\n",
    "        with something else also, because this only needs to be done\n",
    "        on the last step. Therefore the easiest and more efficient way\n",
    "        is to get also the pre-sigmoid activation as an output of\n",
    "        scan, and apply both the log and sigmoid outside scan such\n",
    "        that Theano can catch and optimize the expression.\n",
    "        \"\"\"\n",
    "\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "        :type input: theano.tensor.dmatrix\n",
    "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        # end-snippet-1\n",
    "\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the baises b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyper plain for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of hyper\n",
    "        # plain-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "        .. math::\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-fb2fab8c8b03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mload_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdbn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDBN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'load_data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "\n",
    "from load_data import load_data\n",
    "from dbn import DBN\n",
    "\n",
    "def test_DBN(finetune_lr=0.1, decay=False, training_epochs=100,\n",
    "             pretraining_epochs=10, pretrain_lr=0.01, k=1,\n",
    "             batch_size=10):\n",
    "    \"\"\"\n",
    "    Demonstrates how to train and test a Deep Belief Network.\n",
    "    This is demonstrated on CIFAR-10.\n",
    "    :type finetune_lr: float\n",
    "    :param finetune_lr: learning rate used in the finetune stage\n",
    "    :type decay: boolean\n",
    "    :param decay: whether use weight decay or not in the finetune stage\n",
    "    :type pretraining_epochs: int\n",
    "    :param pretraining_epochs: number of epoch to do pretraining\n",
    "    :type pretrain_lr: float\n",
    "    :param pretrain_lr: learning rate to be used during pre-training\n",
    "    :type k: int\n",
    "    :param k: number of Gibbs steps in CD/PCD\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: maximal number of iterations ot run the optimizer\n",
    "    :type batch_size: int\n",
    "    :param batch_size: the size of a minibatch\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = load_data()\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    numpy_rng = numpy.random.RandomState(123)\n",
    "    print('... building the model')\n",
    "    # construct the Deep Belief Network\n",
    "    dbn = DBN(numpy_rng=numpy_rng, n_ins=32 * 32 * 3,\n",
    "              hidden_layers_sizes=[1000, 1000, 1000],\n",
    "              n_outs=10)\n",
    "\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print('... getting the pretraining functions')\n",
    "    pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size,\n",
    "                                                k=k)\n",
    "\n",
    "    print('... pre-training the model')\n",
    "    start_time = time.clock()\n",
    "    ## Pre-train layer-wise\n",
    "    for i in xrange(dbn.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in xrange(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in xrange(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "            print('Pre-training layer %i, epoch %d, cost ') % (i, epoch),\n",
    "            print (numpy.mean(c))\n",
    "\n",
    "    end_time = time.clock()\n",
    "    # end-snippet-2\n",
    "    print >> sys.stderr, ('The pretraining code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print('... getting the finetuning functions')\n",
    "    train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    print('... finetuning the model')\n",
    "    # early-stopping parameters\n",
    "    patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatches before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    epoch = 0\n",
    "    start_time = time.clock()\n",
    "\n",
    "    while (epoch < training_epochs):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            tlearning_rate = finetune_lr\n",
    "            if decay:\n",
    "                tlearning_rate = finetune_lr/(1.0 + epoch / 10.0)\n",
    "               \n",
    "            minibatch_avg_cost = train_fn(minibatch_index, tlearning_rate)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%, learning rate %f'\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.,\n",
    "                        tlearning_rate\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "\n",
    "    end_time = time.clock()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "    print >> sys.stderr, ('The fine tuning code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time)\n",
    "                                              / 60.))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_DBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
